{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YEP3B08I1OjD",
    "outputId": "9b7e0be7-ca88-484a-d434-94753643d252"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "!pip install textattack\n",
    "!pip install datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2338
    },
    "id": "S1K8vAVoF_Ap",
    "outputId": "b9c3b775-4fca-4db6-a006-ce921878d604"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers torch textattack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1055,
     "referenced_widgets": [
      "418dc3821bbc468fa15b91e8ee3f3d1f",
      "5b51792b9fc5401d98f139dbacb5e22e",
      "837cd1756af4446b8d20a3c05a1e39a2",
      "76c53be353214458a92ec73f7fd59cfa",
      "e97327710bfa46c2be99202083055e76",
      "72b33eb2821b40c583b7c32ddc1744f1",
      "b5e86d705c8740b882008601652886ef",
      "929d0fd4a30346daa11fa6d41bae44a3",
      "3ac7d4a6d245473ab3469edbac2ed1d3",
      "3c7b78d1f99a4e8fb533fec5400aaf50",
      "b430154158684487a84009a853070b88",
      "bc8463861c16454a9035b6a496064136",
      "59428c78f82c4809b1a8919de41bf438",
      "73daeadbf29346b3bf6a546c6ea12b31",
      "7281faa751eb4718b5bf02080ab295a4",
      "8704bf23733e44beb76b871d7665232d",
      "ec50d195dff44a628d924f2618811588",
      "180992633a2647fc87d4088c44dc562b",
      "b392914b10874a9291d05bc0cf60a079",
      "dc62a26b6c9b491ab7b323177fcc26a3",
      "371c77e94282499cbca64fdb1b37e02d",
      "43f0bc28f55a4271b75bf415aeda1132",
      "57c14bae56ff4e4ea4d65ec8178ca7ec",
      "42eb63f77a494d24a2d83afc05c1ab81",
      "66ebe62bfc6b4f98a99ee500291dd0d1",
      "81c02a113a5c42d88e6830139fab9394",
      "57da7a7760ec4c5a92a326673b895575",
      "0dc3100acbc743728abfb8660874fd67",
      "c08266e5d83f4eb3ae824504f7ae7881",
      "4c8111ea7d934fd3b16d5294644ea492",
      "ad71bcf9ffe5424bbfbb492cb57a4ed4",
      "c9fe4e3310e6408e90d7b02dfe47d6b7",
      "3ffcef389a5e4becb8cda4fb6b1b0c9e",
      "74d49c67f0c4402dbd5c6a08d83b05da",
      "180f312c438440feac4d63fb3805b5d3",
      "7180015f985e47f6a17ec2ee440f914f",
      "aaa5ae6df93a46e0a9a0caf85c638b4d",
      "f28e30debaa34921b9f6840ffd0bada3",
      "78eec6f9dca848d4bcc497bde3f91140",
      "a42ec61ad03f45a69fca515b8bf1e726",
      "6524cbdf0f164e538437884aa17c4adb",
      "21351ef932524a52a2eb099d7b0611cd",
      "6133dc0bc0a349d9b253336e55f5dcf3",
      "caad960609de44a5b268b83e338d151c",
      "37a7769502c743099a62ca614f899b5d",
      "337cf961f87f49d99d20d769e8ae1e60",
      "a6cda6c9e2914e2aa67b00895bd61bf2",
      "40f017877ec54a92bb973819bd4b784a",
      "ad97e9fa4fc14855af3d4a57bde400d0",
      "904603af5f7342739cbacad159f9a0ee",
      "b8f8fba2292b4c9da90af70d095ce4b0",
      "16a8c99c707f4d2cac046c24168feb9b",
      "0e2490a1532a44d78fdb37c6957b3f18",
      "5433f228de4d458c873a4dda40c7258a",
      "98985f59a72742d186d4a7a01aa37656",
      "5068603e4af74a89a3b62dbd4cdf5b79",
      "90a8da807a8f49d5b48a83007bc9003a",
      "765e7599c5574e69a1d03cb25ab142f5",
      "57d910fe0f0a4bc1bb2d640e13f77730",
      "15be87fb356f4f08b83af8fde278729a",
      "e3a1416df1ea4671b2761b81c75d4c07",
      "653b69636c904a248fab79838b865f98",
      "bdb0fc53a934472480fc4793c56a7629",
      "bad7dd6f8b194e3c8b5f9a33eb0ee27d",
      "18b79e448cf747e3b88ad651fd71dccb",
      "be13dfe5097949d5b3b66bbc5fd169e8",
      "1112b5cd8e50405291e3f625bcab30d9",
      "388d639430ec447fb19d8c8dbb119e68",
      "56757186bad646c097cac9cdecf5939f",
      "d4fe48af45544a7e969bea8db1b00a05",
      "de64b82b5f6a47dfb6af93c5acd9753f",
      "34dcd8432d6b4618aa1e6749d24ecbd9",
      "0e868ef17e364f338eda42194bcf7a13",
      "5d98e9a8f95d42e58ac74f3d742eaed1",
      "7f51f3c269f04ccea1c94081f52fbc01",
      "d2f5d80a3d3a470694fc738ddd5cf674",
      "4c251d9d20164af68be24ff442d024c1",
      "4b28dc9f49ef410d934ac2a5c582137b",
      "b9dce19f042e4d6a8cc950ac1b7fa718",
      "6f79cf99471f45eaadec5f100b96e160",
      "3fb2a039693543c198c0ccfa5b441286",
      "8afd438038f34486ab430da772b120d7",
      "cc8441b28a7d4d4997489bb7ab72106c",
      "753e62b9b0974acda7d56faaabc1d16c",
      "fc3522e802e44bfd94326f7632fdbd15",
      "4b87df5918ce4c6b994604e477321ddb",
      "3fe08a8c0f824dbdbbde724049c71a6c",
      "23b894a0fc764a889230861b66540f8a",
      "539579c74b2a4eac977a893c46822424",
      "7411fa619bbe4a34a499918effdcd6ba",
      "e7ecca0865ba435db700ee81f18542aa",
      "4c626fdafd3e4e929e700df3d02a7358",
      "2c774aaa77b94c2486890fd16794eaac",
      "5b10853f76d14923a3425d669972febc",
      "ff53c235a3484438a87c1e8e3b817cc5",
      "def63a357f654312957fa05896d18584",
      "cda2e5bf0594490ab7efc2b50d231476",
      "bcc59f1afd22455d88193782f4867905",
      "017864e040cd4515b7181d9103433130",
      "963a974f3459475386d8fba1f5e8cf77",
      "951b527629124076a12333dee8042d8c",
      "f3a9d257324c46fdb4e40864e8085e9c",
      "65f1fb878b30474db9320e31e12600bf",
      "112faf9490ce4a93870aaa549d3ff489",
      "d743e4f881d544fa9c99f8ec966c7872",
      "9103a548ff2147b79682d50a20bfba4d",
      "640bed80583940e2b047aa857656cdca",
      "5b4f59fc6212412596673e1fc97c8c44",
      "7adf3f2ace3a45ba9664e4e1ca7ea830",
      "286d1bf6969748dfa63e394351b9a733",
      "f669da0259bf489bbcdd96ad0d2074c9",
      "7e21f7426f0c403983d3ea75f2fa19d7",
      "c2beddef18c8480b866bf780f7f25aad",
      "b25e73d3789a4d5f9ace3b2920471677",
      "fc30d241e3af4eb58d52b982ae8ffe56",
      "1997647fd5624cc1aac9f81141cc520f",
      "68d6e2ddade04354b3aed0fa6a2eabc1",
      "7d441ac283cb4cfe8845038c2ea94326",
      "3d21414810db4bda84bbf2a4f132f97f",
      "29438b23db8a4b54a80f626ddbd9ff1e",
      "577bff1471ac4952bd33d7fe8e262c45",
      "059c6f2a75b244ac8adbe3b4d25e683f",
      "f8dd3c18fe9a4cb38ef00cf85843350c",
      "17ae8e8362a24198a4205a3617d5a16a",
      "3fd660146b494586b659de5450258a10",
      "7bf9661f4e8c4dd594c692b56ada8691",
      "86f9d77afda640be9127b0ad3b45f342",
      "f479b5a56cfe47c39e52880531921d7d",
      "431f5da4d7334a3081245ea7bb3834a0",
      "1c9bacfcf0744339aabad15debeba3ac",
      "fc3b3792959445e6a8c42fecc57d2349",
      "a8ff7cd78f844f6aba3c1c139b6a0d65",
      "82164b7777714c6daad81e53236d8d47",
      "c331f9aa7eb34260891ff50d4bc1aca3",
      "6395b9d7d10f483580d8f124b455794a",
      "16b433838c384600a5ca24619cdbeb44",
      "b03432974b224b2f83d6e73244cf401a",
      "84f639e372d14d63a0d83c5fd2a66ba7",
      "9cc109ab72e44e6d9959d499f2130462",
      "f757c386fabf41968a1f98e6e1aeee28",
      "32a2f5834d1b429ba3de374e308295f5",
      "9d418969c43840d1a4b20c5616377e18",
      "d37c95bf893348d39b334e50bbe8b8a7",
      "84c221c8d1bf4006a048d20c78ba68ee",
      "388a665226324822a4ef529b6396b2ad",
      "907ba21d3e8d4da68cec28dcce44be07",
      "28be6baaf6394a8cb8ac66c1eb4e4109",
      "d8dd8e85a51b4d2d93f1ad75c0678f83",
      "4c510ae8bc5f49a3b0582843c96b8cbd",
      "21892db8a1294c4f82d82e7d1c87fc29",
      "9b59eab8566646c3b7e2f6f5544cc7bd",
      "796f28a18e724b5f98ea76436df30502",
      "fb21e7a7536f48dca8ddeaa5a08bfc75",
      "da60aae461304944949c26383e49b757",
      "b0059b94af8d428f813515380777283d",
      "d2781eb122b54a7fa28192ce7f003bd0",
      "7d0a042b8e9140a98dc383baef8bbe18",
      "90b4ed0add8148d7b122f4388dba21eb",
      "c6efaca19042433ea1fac659b7311d1c",
      "24dec4fbb03d4ec3bd3e7c7dceac37cf",
      "7c74da91e18a43738cd029f8943bbb4c",
      "2794be3eb51a477c97ae8f49c7f261ef",
      "117540eafafe4c0bbcaafee2a68ad76e",
      "c0e68f2e63fb42d9a1cbfa0fa9cc7621",
      "1b4b1bbd14654020874c507775cfde81"
     ]
    },
    "id": "Fjq-MInY1WW6",
    "outputId": "4830ee74-a709-4af0-8da3-b05bab615d88"
   },
   "outputs": [],
   "source": [
    "#this block only applies textfooler and then created combined dataset, further analysis in next block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and split SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = dataset[\"train\"].select(range(5000))  # Take only 5,000 samples\n",
    "\n",
    "num_samples = len(train_data) // 2\n",
    "clean_data = train_data.select(range(num_samples))\n",
    "perturb_data = train_data.select(range(num_samples, len(train_data)))\n",
    "\n",
    "# Save split datasets\n",
    "clean_data.to_json(\"sst2_clean_split.json\")\n",
    "perturb_data.to_json(\"sst2_perturb_split.json\")\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "classifier = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Wrap model for TextAttack\n",
    "wrapped_model = HuggingFaceModelWrapper(classifier, tokenizer)\n",
    "\n",
    "# Initialize TextFooler Attack\n",
    "attack = TextFoolerJin2019.build(wrapped_model)\n",
    "\n",
    "# Apply adversarial attack\n",
    "perturbed_texts = []\n",
    "for example in tqdm(perturb_data, desc=\"Applying TextFooler Attack\"):\n",
    "    input_text = example['sentence']\n",
    "    ground_truth_label = example['label']\n",
    "    try:\n",
    "        adv_example = attack.attack(input_text, ground_truth_label)\n",
    "        adv_text = adv_example.perturbed_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating adversarial example: {e}\")\n",
    "        adv_text = input_text\n",
    "    perturbed_texts.append({\"sentence\": adv_text, \"label\": ground_truth_label})\n",
    "\n",
    "# Convert to Dataset format\n",
    "perturbed_dataset = Dataset.from_list(perturbed_texts)\n",
    "perturbed_dataset.to_json(\"sst2_textfooler_split.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "924fd423ab2e454ab8c4652794807ddc",
      "b71cf9d915d74ec68ebf75d7d4622aeb",
      "ee1a93e2e27a4cc0863fdc20a23ae373",
      "f07b325bd09343358a5dee6e4ed67ba2",
      "f8a72a3096b34e7faf9bdfe50fe89525",
      "f1571c861dd74bcea0aaa6962fad5e41",
      "4ffdd4124c144fa5899c35457b190149",
      "12e49b64a5a24850a881fb1536509544",
      "1003230d0ca14689bde657ecfa3c0272",
      "00c413b64a3449b6a0575bde52c76d08",
      "2dc926fba23c4532bc67c91b812e2f3e",
      "3dd0261cd5de4572ba1d0dd2fc87ff97",
      "e431b1cabdb64323a66dd9265a84ab8a",
      "6589809847834c95852574cfdfc7797c",
      "4428836a2ed74ea7a04b4b35f0d5030c",
      "a5c0b02af21f4758bee53d388cdf6330",
      "59e5988549364dc889102a1f911a5767",
      "06d0d3707cb740b3b69170ab282c9581",
      "c07550fe0b0641d5b72a76be3473ce21",
      "8fde4cd8c4b84851acc579cfc3ad39f8",
      "927481a5f24f410f8118dc8cbce8c8fb",
      "1f5fc3a978274e72a5ab702c99a268ee",
      "cae8ff51ca3a42a48ce5d4c9d73823d1",
      "0f78fdd784134c03919b374a113f9a5d",
      "19be7decb0ae440da00d5d8e537dbab1",
      "f737e95635764a64ad2bb563e56d3232",
      "fd92e176d9724551a39252c8fd626030",
      "611b6ef435c34481aa45f96e1ede66a0",
      "df28ae773485437890bf67b7d63c9856",
      "ae8844da418d4c788704cfb68cc1c05d",
      "e80b7402b1924c59a18efe901ac7dfda",
      "0ae9f2ec1313473fabfd2f2864aecfb5",
      "02f0d667dc224ca6928b64bdc901d94e"
     ]
    },
    "id": "GPK78JAQ1lO3",
    "outputId": "cef4c516-4030-4e36-8dd6-bd185bcbe9a8"
   },
   "outputs": [],
   "source": [
    "# Merge the clean and perturbed datasets to generate balanced dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "clean_data = Dataset.from_json(\"sst2_clean_split.json\")\n",
    "perturbed_data = Dataset.from_json(\"sst2_textfooler_split.json\")\n",
    "\n",
    "num_samples = min(len(clean_data), len(perturbed_data))\n",
    "clean_data = clean_data.select(range(num_samples))\n",
    "perturbed_data = perturbed_data.select(range(num_samples))\n",
    "\n",
    "# Convert to lists and filter out invalid entries\n",
    "clean_list = [ex for ex in clean_data.to_list() if ex[\"sentence\"] is not None]\n",
    "perturbed_list = [ex for ex in perturbed_data.to_list() if ex[\"sentence\"] is not None]\n",
    "\n",
    "# Merge clean and perturbed datasets\n",
    "final_dataset = Dataset.from_list(clean_list + perturbed_list)\n",
    "final_dataset.to_json(\"sst2_final_balanced_textfooler.json\")\n",
    "\n",
    "print(\"wFinal balanced dataset (50% clean, 50% TextFooler-attacked) saved as 'sst2_final_balanced_textfooler.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Jf3g0jC1lLQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4Wefr5P1k1G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCXBPgr21WZC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6263,
     "referenced_widgets": [
      "e12940568f324c1eacc92cb7e3f5ebe1",
      "fbe0f719e4ad400ebc2f07ab162b2661",
      "4951eafd00264402873d24eb386dbd3b",
      "5c425beeded0429a8c9ef089b3ac6ff9",
      "45923451f64740fc9f0489fb61a340c5",
      "f17cc5f9acd34c2b843cf02f0004db21",
      "116888ef7aaf4123b6ed289d2bc78b06",
      "f1387d78b6ef4b4db92e7e813b7f4568",
      "5aedcfc49ccf4370b5360153c37140d2",
      "f128f32c37d2413086d0a4e4aa040591",
      "3aede1487a8d4520a03343ed3b864cb5",
      "afd75954d5cb4aaeb5d71140e9e805bd",
      "d3cf721f380445659904cfd4f7ba0d6b",
      "1d1146228d27443e8b53ae74820e89b5",
      "7f80744124ce4bf291afc60d21b74f6c",
      "3c7ea88a064f415ba895e98257a6b802",
      "4bcb256c098048de8c9af8c2dd49235b",
      "4a62d11218254cb8bdae9d03ff6b0ae6",
      "36c35731ff1046838a60292ebe4dd81e",
      "55a07123db9c430cb4588e44baae61c8",
      "9b2d55e9d7db414f803cf008ce557097",
      "05c9a101786c4a928dbef205fff9202c",
      "f977126fd2654b0c908b54d13ed31ecb",
      "cc438797315e40fa9b07bb3f0686b226",
      "defb40aaefa540a9b1d7ff77da42aeb0",
      "9aa394d7f33243b88ac4c5bf960bb878",
      "34cf389c2d6d46f48c8987b023098817",
      "aea226e0873a49e08c5b656e27b21a21",
      "5fd9539f759841edb3fe550b27437d4d",
      "bd088469c3d041f8831ea7df8b421e78",
      "cde8178c71004351baaeca68e4e9be4c",
      "cb5c5894b981433c8a97f33ab136625e",
      "93c0e4ab16d14f11a0b7727c90ecba6f"
     ]
    },
    "id": "1qE5kvXz1Wch",
    "outputId": "729edc08-a85c-4734-cb7c-15c737a384af"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Load combined dataset after TextFooler attack\n",
    "final_dataset = Dataset.from_json(\"sst2_final_balanced_textfooler.json\")\n",
    "print(\"Dataset columns:\", final_dataset.column_names)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    if \"sentence\" not in examples:\n",
    "        raise ValueError(\"Dataset does not contain a 'sentence' column. Available columns: \" + str(final_dataset.column_names))\n",
    "    return tokenizer([str(s) for s in examples[\"sentence\"]], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Filter dataset to remove invalid inputs\n",
    "filtered_dataset = final_dataset.filter(lambda x: isinstance(x[\"sentence\"], str) and x[\"sentence\"].strip() != \"\")\n",
    "\n",
    "# Apply tokenization\n",
    "try:\n",
    "    tokenized_dataset = filtered_dataset.map(tokenize_function, batched=True)\n",
    "except ValueError as e:\n",
    "    print(\"Error during tokenization:\", e)\n",
    "    print(\"Example dataset structure:\", filtered_dataset[0])  # Debugging info\n",
    "    exit()\n",
    "\n",
    "# Define model\n",
    "classifier = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sst2_trained_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "SAVE_PATH = \"sst2_trained_model\"\n",
    "classifier.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f\"Trained model saved to '{SAVE_PATH}'\")\n",
    "\n",
    "# Function to extract logits\n",
    "def get_logits(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(classifier.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(**inputs)\n",
    "    return outputs.logits.squeeze().cpu().numpy()\n",
    "\n",
    "# Function to extract embeddings from hidden layers\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512).to(classifier.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[-1].mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to compute logit and embedding differences\n",
    "def compute_detection_features(original_text, adversarial_text):\n",
    "    orig_logits = get_logits(original_text)\n",
    "    adv_logits = get_logits(adversarial_text)\n",
    "\n",
    "    logit_diff = float(np.linalg.norm(orig_logits - adv_logits))  # Logit difference\n",
    "\n",
    "    orig_embedding = get_embedding(original_text)\n",
    "    adv_embedding = get_embedding(adversarial_text)\n",
    "    embed_similarity = float(1 - cosine(orig_embedding, adv_embedding))  # Embedding similarity\n",
    "\n",
    "    return logit_diff, embed_similarity\n",
    "\n",
    "# Evaluate detection on final dataset\n",
    "detection_results = []\n",
    "for example in tqdm(filtered_dataset, desc=\"Evaluating Detection\"):\n",
    "    text = example['sentence']\n",
    "    label = example['label']\n",
    "    logit_diff, embed_similarity = compute_detection_features(text, text)\n",
    "    detected = int(logit_diff > 0.08 or embed_similarity < 0.75)  # Explicit detection flag\n",
    "    detection_results.append({\n",
    "        \"text\": text,\n",
    "        \"logit_diff\": logit_diff,\n",
    "        \"embed_similarity\": embed_similarity,\n",
    "        \"label\": label,\n",
    "        \"detected\": detected\n",
    "    })\n",
    "\n",
    "# Save detection results\n",
    "with open(\"sst2_adversarial_detection_results.json\", \"w\") as f:\n",
    "    json.dump(detection_results, f, indent=4)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "LOGIT_DIFF_VALUES = np.arange(0.1, 0.5, 0.08)\n",
    "EMBED_SIM_VALUES = np.arange(0.7, 0.9, 0.08)\n",
    "best_f1, best_auc = 0, 0\n",
    "best_thresholds = (None, None)\n",
    "all_tuning_results = []\n",
    "\n",
    "for logit_thresh, embed_thresh in product(LOGIT_DIFF_VALUES, EMBED_SIM_VALUES):\n",
    "    for res in detection_results:\n",
    "        res[\"detected\"] = res[\"logit_diff\"] > logit_thresh or res[\"embed_similarity\"] < embed_thresh\n",
    "\n",
    "    true_labels = [res[\"label\"] for res in detection_results]\n",
    "    predicted_labels = [1 if res[\"detected\"] else 0 for res in detection_results]\n",
    "\n",
    "    if len(set(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    else:\n",
    "        auc = 0\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=1)\n",
    "    all_tuning_results.append({\n",
    "        \"logit_threshold\": logit_thresh,\n",
    "        \"embed_threshold\": embed_thresh,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"auc\": auc\n",
    "    })\n",
    "\n",
    "    if f1 > best_f1 or (f1 == best_f1 and auc > best_auc):\n",
    "        best_f1 = f1\n",
    "        best_auc = auc\n",
    "        best_thresholds = (logit_thresh, embed_thresh)\n",
    "\n",
    "# Compute ASR\n",
    "misclassified_adversarial = sum(1 for res in detection_results if res[\"label\"] == 1 and res[\"detected\"] == 0)\n",
    "total_adversarial = sum(1 for res in detection_results if res[\"label\"] == 1)\n",
    "asr = misclassified_adversarial / total_adversarial if total_adversarial > 0 else 0\n",
    "print(f\"Attack Success Rate (ASR): {asr:.2%}\")\n",
    "\n",
    "# Save tuning results\n",
    "with open(\"sst2_threshold_tuning_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"best_thresholds\": best_thresholds,\n",
    "        \"best_f1_score\": best_f1,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"attack_success_rate\": asr,\n",
    "        \"all_results\": all_tuning_results\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(\"Best thresholds, Accuracy, F1 Score, AUC, and Attack Success Rate saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
